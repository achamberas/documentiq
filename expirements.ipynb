{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vertexai\n",
      "  Downloading vertexai-1.60.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-aiplatform==1.60.0 (from google-cloud-aiplatform[all]==1.60.0->vertexai)\n",
      "  Downloading google_cloud_aiplatform-1.60.0-py2.py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.19.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.27.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (3.24.0)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai)\n",
      "  Downloading google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai)\n",
      "  Downloading shapely-2.0.5-cp39-cp39-macosx_10_9_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: pydantic<3 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.10.12)\n",
      "Collecting docstring-parser<1 (from google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "\u001b[33mWARNING: google-cloud-aiplatform 1.60.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.59.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.60.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from pydantic<3->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (4.9.0)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.26.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (0.4.8)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.60.0->google-cloud-aiplatform[all]==1.60.0->vertexai) (2022.12.7)\n",
      "Downloading vertexai-1.60.0-py3-none-any.whl (7.3 kB)\n",
      "Downloading google_cloud_aiplatform-1.60.0-py2.py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.9/341.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.0.5-cp39-cp39-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shapely, docstring-parser, google-cloud-resource-manager, google-cloud-aiplatform, vertexai\n",
      "Successfully installed docstring-parser-0.16 google-cloud-aiplatform-1.60.0 google-cloud-resource-manager-1.12.5 shapely-2.0.5 vertexai-1.60.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU pypdf langchain langchain_community langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create link to external models in BigQuery\n",
    "\n",
    "Based off of this article\n",
    "\n",
    "https://cloud.google.com/blog/products/data-analytics/how-to-use-rag-in-bigquery-to-bolster-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f8af97e17f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import google.auth\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# authenticate to Google Cloud\n",
    "\n",
    "GOOGLE_PROJECT = 'gristmill5'\n",
    "credentials = service_account.Credentials.from_service_account_file(\"creds/gristmill5-e521e2f08f35.json\")\n",
    "client = bigquery.Client(GOOGLE_PROJECT, credentials)\n",
    "\n",
    "# create link to embedding model\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE MODEL `gristmill5.rag_test.gecko_embedding_model`\n",
    "REMOTE WITH CONNECTION `projects/gristmill5/locations/us/connections/vertex_ai`\n",
    "OPTIONS (ENDPOINT = 'textembedding-gecko');\n",
    "\"\"\"\n",
    "\n",
    "client.query(sql, project=GOOGLE_PROJECT).result()\n",
    "\n",
    "# create link to LLM\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE MODEL `gristmill5.rag_test.gemini_llm_model`\n",
    "REMOTE WITH CONNECTION `projects/gristmill5/locations/us/connections/vertex_ai`\n",
    "OPTIONS (ENDPOINT = 'gemini-1.0-pro');\n",
    "\"\"\"\n",
    "\n",
    "client.query(sql, project=GOOGLE_PROJECT).result()\n",
    "\n",
    "# create table function that accepts a user query, finds similar chunks, and passes those chunks to the LLM\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE FUNCTION rag_test.rag_query(querys STRING, route_type STRING, words INT64, doc_source ARRAY <STRING>, selected_distance FLOAT64) AS (\n",
    "with q_embeddings as (\n",
    "  SELECT\n",
    "    text_embedding,\n",
    "    content\n",
    "  FROM\n",
    "    ML.GENERATE_TEXT_EMBEDDING(\n",
    "      MODEL `rag_test.gecko_embedding_model`,\n",
    "      (\n",
    "        SELECT\n",
    "        CAST(querys AS STRING) AS content\n",
    "      )\n",
    "    )\n",
    "),\n",
    "\n",
    "a_embeddings as (\n",
    "  select * \n",
    "  from `rag_test.embeddings` \n",
    "  where source in UNNEST(doc_source)\n",
    "  and embedding_type = FORMAT('%s', route_type)\n",
    "  and FORMAT('%s', route_type) = 'summary'\n",
    "),\n",
    "\n",
    "v_search as (\n",
    "  SELECT *\n",
    "  FROM\n",
    "    VECTOR_SEARCH( \n",
    "      (\n",
    "        select * \n",
    "        from `rag_test.embeddings` \n",
    "        where source in UNNEST(doc_source)\n",
    "        -- and statistics is not null\n",
    "        and embedding_type = FORMAT('%s', route_type)\n",
    "        and FORMAT('%s', route_type) = 'details'\n",
    "      ),\n",
    "      'text_embedding',\n",
    "      (select * from q_embeddings where 1=1),\n",
    "      top_k => 5\n",
    "    )\n",
    "  WHERE distance < selected_distance\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM \n",
    "  ML.GENERATE_TEXT(\n",
    "    MODEL\n",
    "      `rag_test.gemini_llm_model`, \n",
    "      (\n",
    "        -- query for when an answer needs to contain specific details\n",
    "        SELECT\n",
    "          CONCAT(FORMAT('Answer this question in less than %d words:\\\\n\\\\n %s \\\\n\\\\n', words, querys), '\\\\n\\\\n by using these text chunks: \\\\n\\\\n', STRING_AGG(base.chunk, '\\\\n')) AS prompt, \n",
    "          -- CONCAT(FORMAT('Summarize these text chunks in less than %d words:\\\\n\\\\n', words), STRING_AGG(base.chunk, '\\\\n')) AS prompt, \n",
    "          ARRAY_AGG(\n",
    "            STRUCT(\n",
    "              base.id as id,\n",
    "              base.chunk as chunk,\n",
    "              -- base.statistics as statistics, \n",
    "              base.embedding_type,\n",
    "              -- base.ml_embed_text_status as status,\n",
    "              distance as distance\n",
    "            )\n",
    "          ) source_ids\n",
    "        FROM v_search\n",
    "\n",
    "        -- query for when answer needs to be a summary\n",
    "        UNION ALL SELECT \n",
    "          CONCAT(FORMAT('Summarize this text in less than %d words:\\\\n\\\\n', words), SUBSTRING(chunk, 1, 32760)) AS prompt, \n",
    "          [\n",
    "            STRUCT(\n",
    "              id,\n",
    "              chunk,\n",
    "              -- statistics, \n",
    "              embedding_type,\n",
    "              -- {} as status,\n",
    "              0.1 as distance\n",
    "            )\n",
    "          ] source_ids\n",
    "        FROM a_embeddings\n",
    "      ),\n",
    "      STRUCT(\n",
    "        0.4 AS temperature,\n",
    "        300 AS max_output_tokens,\n",
    "        0.5 AS top_p,\n",
    "        5 AS top_k,\n",
    "        TRUE AS flatten_json_output\n",
    "      )\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "client.query(sql, project=GOOGLE_PROJECT).result()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Hacker News - 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f7d88bcff10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "INSERT INTO TABLE `rag_test.embeddings` as\n",
    "SELECT 'Ask HN' as source, *\n",
    "FROM ML.GENERATE_TEXT_EMBEDDING(\n",
    "  MODEL `rag_test.gecko_embedding_model`, (\n",
    "    SELECT cast(id AS STRING) id, concat(title, ': ', text) as content \n",
    "    FROM `bigquery-public-data.hacker_news.full` \n",
    "    where text is not null\n",
    "    and type = 'story'\n",
    "    and timestamp > '2024-01-01'\n",
    "    )\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "client.query(sql, project=GOOGLE_PROJECT).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "conn = sqlite3.connect('/Users/anthonychamberas/chat.db')\n",
    "# conn = sqlite3.connect('~/Library/Messages/chat.db')\n",
    "\n",
    "messages = pd.read_sql_query('''\n",
    "    select distinct h.id phone_number, chj.chat_id, m.ROWID message_id, m.text, m.attributedBody, HEX(m.attributedBody) hex_message, m.date, m.handle_id, datetime(m.date/1000000000 + strftime(\"%s\", \"2001-01-01\") ,\"unixepoch\",\"localtime\") as date_utc \n",
    "    from chat_handle_join chj \n",
    "    inner join chat_message_join cmj \n",
    "        on chj.chat_id = cmj.chat_id \n",
    "        -- and chj.handle_id in (7,8,9)\n",
    "    inner join message m \n",
    "        on cmj.message_id = m.ROWID \n",
    "    inner join handle h \n",
    "        on chj.handle_id = h.ROWID \n",
    "''', conn)\n",
    "\n",
    "mapping =  dict.fromkeys(range(32))\n",
    "\n",
    "messages['hex_message'] = messages['hex_message'].apply(lambda x: bytes.fromhex(x))\n",
    "messages['decoded'] = messages['hex_message'].str.decode(\"utf-8\", \"ignore\")\n",
    "messages['cleaned'] = messages['decoded'].str.translate(mapping)\n",
    "messages['stripped'] = messages['cleaned'].str.extract(r'\\+(.*)iI')\n",
    "\n",
    "messages['reps'] = messages['stripped'].str.extract(r'(\\dx\\d*)')\n",
    "messages['dots'] = messages['stripped'].str.extract(r'[….|…|..|…|..\\s|.. ](\\d*)')\n",
    "messages['comma'] = messages['stripped'].str.extract(r'(\\d*)[,]')\n",
    "\n",
    "#messages[['date_utc', 'chat_id', 'handle_id','stripped', 'reps', 'dots']].to_csv('cleaned.csv')\n",
    "messages[['date_utc', 'phone_number', 'chat_id', 'handle_id','stripped', 'reps', 'dots']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import streamlist as st\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = st.secrets[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embed = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "chunk = ['embed this sentence.', 'search for this sentence']\n",
    "df = pd.DataFrame(chunk, columns=['chunk'])\n",
    "\n",
    "vectors = embed.embed_documents(df['chunk'])\n",
    "df['vectors'] = pd.Series(vectors).to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "GOOGLE_PROJECT = 'gristmill5'\n",
    "credentials = service_account.Credentials.from_service_account_file(\"creds/gristmill5-e521e2f08f35.json\")\n",
    "client = bigquery.Client(GOOGLE_PROJECT, credentials)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(autodetect=True)\n",
    "#table_id = bigquery.Table('table') \n",
    "#table_id = client.create_table(table, exists_ok=True)\n",
    "\n",
    "job = client.load_table_from_dataframe(df,\"gristmill5.rag_test.table_id\",job_config=job_config).result()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tableau Zen - Visual Analytics Maturity Assess...</td>\n",
       "      <td>0</td>\n",
       "      <td>Why does it matter to me?</td>\n",
       "      <td>0.280168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tableau Zen - Visual Analytics Maturity Assess...</td>\n",
       "      <td>0</td>\n",
       "      <td>How does it work?</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  page  \\\n",
       "8  Tableau Zen - Visual Analytics Maturity Assess...     0   \n",
       "4  Tableau Zen - Visual Analytics Maturity Assess...     0   \n",
       "\n",
       "                       chunk  similarity  \n",
       "8  Why does it matter to me?    0.280168  \n",
       "4          How does it work?    0.999999  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.connectors import *\n",
    "\n",
    "sql = f\"\"\"\n",
    "    select * \n",
    "    from `rag_test.embeddings` \n",
    "    where source in UNNEST(['Tableau Zen - Visual Analytics Maturity Assessment.docx'])\n",
    "\"\"\"\n",
    "\n",
    "data = bq_conn(sql)\n",
    "\n",
    "query = 'How does it work?'\n",
    "query = query.replace(\"'\", \"\\\\'\")\n",
    "embed = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector = embed.embed_documents([query])\n",
    "\n",
    "# Calculate cosine similarities between the query vector and the dataset\n",
    "vectors = np.array(data['vectors'].to_list())\n",
    "# similarities = cosine_similarity(vectors, vector)\n",
    "\n",
    "# similarity_array = [s[0] for s in similarities]\n",
    "# similarity_df = pd.DataFrame(similarity_array, columns=['similarity'])\n",
    "similarities = pd.DataFrame([s[0] for s in cosine_similarity(vectors, vector)], columns=['similarity'])\n",
    "df = pd.concat([data, similarities], axis=1)\n",
    "\n",
    "n = 2\n",
    "# top_n_idx = np.argsort(similarity_array)[-n:]\n",
    "top_n_idx = np.argsort(df['similarity'])[-n:]\n",
    "references = df[['source', 'page', 'chunk', 'similarity']].iloc[top_n_idx]\n",
    "\n",
    "display(references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ics-vtimezones\n",
      "  Downloading ics_vtimezones-2020.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting importlib_resources>=1.4 (from ics-vtimezones)\n",
      "  Downloading importlib_resources-6.4.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.9/site-packages (from importlib_resources>=1.4->ics-vtimezones) (3.20.0)\n",
      "Downloading ics_vtimezones-2020.2-py3-none-any.whl (184 kB)\n",
      "Downloading importlib_resources-6.4.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: importlib_resources, ics-vtimezones\n",
      "Successfully installed ics-vtimezones-2020.2 importlib_resources-6.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ics-vtimezones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ics import Calendar, Event\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Define the timezone (e.g., for Eastern Time)\n",
    "timezone = pytz.timezone('America/New_York')\n",
    "\n",
    "# List of events with their respective dates and descriptions\n",
    "events = [\n",
    "    {\"name\": \"Teachers' Professional Learning\", \"start\": \"2024-08-26\", \"end\": \"2024-08-27\"},\n",
    "    {\"name\": \"First day for students (K-7, 9)\", \"start\": \"2024-08-28\"},\n",
    "    {\"name\": \"First day for students (8, 10-12)\", \"start\": \"2024-08-29\"},\n",
    "    {\"name\": \"Labor Day: NO SCHOOL\", \"start\": \"2024-09-02\"},\n",
    "    {\"name\": \"Rosh Hashanah: NO SCHOOL\", \"start\": \"2024-10-03\"},\n",
    "    {\"name\": \"Indigenous Peoples' Day: NO SCHOOL\", \"start\": \"2024-10-14\"},\n",
    "    {\"name\": \"Diwali: NO SCHOOL\", \"start\": \"2024-11-01\"},\n",
    "    {\"name\": \"Professional Day: NO SCHOOL\", \"start\": \"2024-11-05\"},\n",
    "    {\"name\": \"Veterans Day: NO SCHOOL\", \"start\": \"2024-11-11\"},\n",
    "    {\"name\": \"Thanksgiving: NO SCHOOL\", \"start\": \"2024-11-28\"},\n",
    "    {\"name\": \"Schools closed; Offices closed\", \"start\": \"2024-11-29\"},\n",
    "    {\"name\": \"Winter Vacation (Offices open)\", \"start\": \"2024-12-23\", \"end\": \"2025-01-01\"},\n",
    "    {\"name\": \"Christmas: NO SCHOOL\", \"start\": \"2024-12-25\"},\n",
    "    {\"name\": \"New Year's Day: NO SCHOOL\", \"start\": \"2025-01-01\"},\n",
    "    {\"name\": \"Martin Luther King Jr. Day: NO SCHOOL\", \"start\": \"2025-01-20\"},\n",
    "    {\"name\": \"Lunar New Year: NO SCHOOL\", \"start\": \"2025-01-29\"},\n",
    "    {\"name\": \"Presidents' Day: NO SCHOOL\", \"start\": \"2025-02-17\"},\n",
    "    {\"name\": \"February Vacation (Offices open)\", \"start\": \"2025-02-17\", \"end\": \"2025-02-21\"},\n",
    "    {\"name\": \"Eid al-Fitr: NO SCHOOL\", \"start\": \"2025-03-31\"},\n",
    "    {\"name\": \"Good Friday: NO SCHOOL\", \"start\": \"2025-04-18\"},\n",
    "    {\"name\": \"Patriots Day: NO SCHOOL\", \"start\": \"2025-04-21\"},\n",
    "    {\"name\": \"April Vacation (Offices open)\", \"start\": \"2025-04-21\", \"end\": \"2025-04-25\"},\n",
    "    {\"name\": \"Memorial Day: NO SCHOOL\", \"start\": \"2025-05-26\"},\n",
    "    {\"name\": \"ABRHS Graduation\", \"start\": \"2025-06-06\"},\n",
    "    {\"name\": \"180th Day - Last Day/Early Release if no cancellations\", \"start\": \"2025-06-18\"},\n",
    "    {\"name\": \"Juneteenth: NO SCHOOL\", \"start\": \"2025-06-19\"},\n",
    "    {\"name\": \"185th Day (hold for possible cancellations)\", \"start\": \"2025-06-26\"}\n",
    "]\n",
    "\n",
    "# Create a new calendar\n",
    "calendar = Calendar()\n",
    "\n",
    "# Add events to the calendar\n",
    "for event in events:\n",
    "    e = Event()\n",
    "    e.name = event[\"name\"]\n",
    "    e.begin = timezone.localize(datetime.strptime(event[\"start\"], \"%Y-%m-%d\"))\n",
    "    if \"end\" in event:\n",
    "        e.end = timezone.localize(datetime.strptime(event[\"end\"], \"%Y-%m-%d\"))\n",
    "    else:\n",
    "        e.make_all_day()\n",
    "    calendar.events.add(e)\n",
    "\n",
    "# Add Early Dismissal events\n",
    "early_dismissal_dates = [\n",
    "    \"2024-09-16\", \"2024-10-07\", \"2024-10-21\", \"2024-11-18\", \"2024-12-09\",\n",
    "    \"2025-01-06\", \"2025-01-27\", \"2025-02-10\", \"2025-02-24\", \"2025-03-10\",\n",
    "    \"2025-03-24\", \"2025-04-07\", \"2025-04-28\", \"2025-05-05\", \"2025-05-19\"\n",
    "]\n",
    "\n",
    "for date in early_dismissal_dates:\n",
    "    e = Event()\n",
    "    e.name = \"Early Dismissal\"\n",
    "    e.begin = timezone.localize(datetime.strptime(date + \" 13:15\", \"%Y-%m-%d %H:%M\"))\n",
    "    e.end = timezone.localize(datetime.strptime(date + \" 14:15\", \"%Y-%m-%d %H:%M\"))\n",
    "    calendar.events.add(e)\n",
    "\n",
    "# Write the calendar to an .ics file\n",
    "with open('2024-2025_ABRHS_Academic_Calendar.ics', 'w') as f:\n",
    "    f.writelines(calendar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \"\"\"\n",
    "            upload_file_name = uploaded_file.name\n",
    "            upload_file_path = f'docs/tmp_{upload_file_name}'\n",
    "            upload_file_data = uploaded_file.read()\n",
    "            upload_file_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "            if uploaded_file.type == 'text/plain':\n",
    "                # save file locally\n",
    "                with open(upload_file_path, 'wb') as uf:\n",
    "                    uf.write(upload_file_data)\n",
    "\n",
    "                loader = TextLoader(upload_file_path)\n",
    "                loader_docs = loader.load()\n",
    "\n",
    "                docs = []\n",
    "                for d in loader_docs:\n",
    "                    docs.append({'page':0, 'page_content':d.page_content})\n",
    "\n",
    "            elif uploaded_file.type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':\n",
    "                # save file locally\n",
    "                with open(upload_file_path, 'wb') as uf:\n",
    "                    uf.write(upload_file_data)\n",
    "\n",
    "                loader = Docx2txtLoader(upload_file_path)\n",
    "                loader_docs = loader.load()\n",
    "\n",
    "                docs = []\n",
    "                for d in loader_docs:\n",
    "                    docs.append({'page':0, 'page_content':d.page_content})\n",
    "\n",
    "            elif uploaded_file.type == 'application/pdf':\n",
    "                # save file locally\n",
    "                with open(upload_file_path, 'wb') as uf:\n",
    "                    uf.write(upload_file_data)\n",
    "\n",
    "                reader = PdfReader(upload_file_path)\n",
    "                extract_images = False if len(reader.pages[0].extract_text()) > 0 else True\n",
    "\n",
    "                loader_message = \"Loading document with OCR...\" if extract_images else \"Loading document...\"\n",
    "                loader = PyPDFLoader(upload_file_path, extract_images=extract_images)\n",
    "                loader_docs = loader.load()\n",
    "\n",
    "                docs = []\n",
    "                for d in loader_docs:\n",
    "                    docs.append({'page':d.metadata['page'], 'page_content':d.page_content})\n",
    "\n",
    "            elif uploaded_file.type == 'application/json':\n",
    "                docs = json.loads(uploaded_file.getvalue())\n",
    "                df = pd.DataFrame(docs)\n",
    "\n",
    "                st.multiselect(label='Select the keys from the JSON file that you\\'d like included in the documents.', options=df.columns, key='json_keys')\n",
    "                selected_columns = st.session_state['json_keys']\n",
    "                df = df[selected_columns]\n",
    "                df['page_content'] = df.apply(lambda x: '\\n'.join([selected_columns[i]+': '+x[i] for i in range(0, len(x))]), axis=1)\n",
    "                df['page'] = df.index\n",
    "                df['metadata'] = df['page'].apply(lambda x: {'page':x})\n",
    "\n",
    "                docs = df.to_dict(orient='records')\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "                    \"\"\"\n",
    "                    text_splitter = SemanticChunker(\n",
    "                        OpenAIEmbeddings(), \n",
    "                        breakpoint_threshold_type=\"percentile\",\n",
    "                        breakpoint_threshold_amount=0.1,\n",
    "                        # number_of_chunks=30\n",
    "                    )\n",
    "\n",
    "                    splits_text = []\n",
    "\n",
    "                    for d in docs:\n",
    "                        page_content = d['page_content']\n",
    "                        page = d['page']\n",
    "\n",
    "                        splits = text_splitter.create_documents([page_content])\n",
    "                        splits_text.extend([{'id': upload_file_id, 'source':upload_file_name, 'page':page, 'embedding_type':'details', 'split_type':split_type, 'chunk': d.dict()['page_content']} for d in splits])\n",
    "\n",
    "                    df = pd.DataFrame(splits_text)\n",
    "                    \"\"\"\n",
    "\n",
    "                    \"\"\"\n",
    "                    # create embeddings\n",
    "                    embed = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "                    vectors = embed.embed_documents(df['chunk'])\n",
    "                    df['vectors'] = pd.Series(vectors).to_numpy()\n",
    "                    df = df.reset_index()\n",
    "                    \"\"\"\n",
    "\n",
    "\n",
    "                    \"\"\"\n",
    "                    # summarize document using map-reduce\n",
    "                    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "                    \n",
    "                    # Map\n",
    "                    map_template = \"\"The following is a set of documents\n",
    "                    {docs}\n",
    "                    Based on this list of docs, please identify the main themes in 300 or fewer words\n",
    "                    Helpful Answer:\"\"\n",
    "                    map_prompt = PromptTemplate.from_template(map_template)\n",
    "                    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "                    summary = map_chain.run(docs)\n",
    "                    \"\"\"\n",
    "\n",
    "                    \"\"\"\n",
    "                    # load embeddings to big query\n",
    "                    GOOGLE_PROJECT = 'gristmill5'\n",
    "                    credentials = service_account.Credentials.from_service_account_file(\"creds/gristmill5-e521e2f08f35.json\")\n",
    "                    client = bigquery.Client(GOOGLE_PROJECT, credentials)\n",
    "                    job_config = bigquery.LoadJobConfig(autodetect=True)\n",
    "\n",
    "                    job = client.load_table_from_dataframe(df,\"gristmill5.rag_test.embeddings\",job_config=job_config).result()\n",
    "\n",
    "                    # load document contents and summary to big query\n",
    "                    docs_df = pd.DataFrame([[upload_file_id, friendly_name, upload_file_name, uploaded_file.type, upload_file_data, summary]], columns=['id', 'name', 'filename', 'filetype', 'contents', 'summary'])\n",
    "                    client.load_table_from_dataframe(docs_df,\"gristmill5.rag_test.documents\",job_config=job_config).result()\n",
    "                    if os.path.exists(upload_file_path):\n",
    "                        os.remove(upload_file_path)\n",
    "                    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
